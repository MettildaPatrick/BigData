{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MettildaBatch104Project",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dbed85L5DITE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2mgEkFrolTS"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "zjZ_nQLgo6Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content//spark-3.0.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "zdZgoNn9o6XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "a1vYe_Eco6Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import * "
      ],
      "metadata": {
        "id": "h8IanuFYo6ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "Mt38tpDyo6ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5XwexDIzo6hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "9DsuV60Fo6kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SparkProject').getOrCreate()"
      ],
      "metadata": {
        "id": "RZlHE3mro6nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "yES0CtvFqZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ByrDpaaYPof2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the schema for all the dataset and use the schema to read the file as DF."
      ],
      "metadata": {
        "id": "ncWgSJY1Pr6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import * \n",
        "\n",
        "Schema_OD = StructType([\n",
        "      StructField('Order_channel_code',IntegerType(),True),\n",
        "      StructField('Order_channel_type',StringType(),True),    \n",
        "])\n",
        "\n",
        "\n",
        "Schema_PO = StructType([\n",
        "      StructField('Retailer_code',IntegerType(),True),\n",
        "      StructField('product_number',IntegerType(),True),\n",
        "      StructField('Date',StringType(),True),\n",
        "      StructField('Quantity',IntegerType(),True)\n",
        "])\n",
        "\n",
        "\n",
        "Schema_P = StructType([\n",
        "      StructField('product_number',IntegerType(),True),\n",
        "      StructField('Product_line',StringType(),True),\n",
        "      StructField('Product_type',StringType(),True),\n",
        "      StructField('Product',StringType(),True),\n",
        "      StructField('Product_brand',StringType(),True),\n",
        "      StructField('Product_colour',StringType(),True),\n",
        "      StructField('Unit_loss',FloatType(),True),\n",
        "      StructField('Unit_price',FloatType(),True),\n",
        "])\n",
        "\n",
        "\n",
        "Schema_S = StructType([\n",
        "      StructField('Retailer_code',IntegerType(),True),\n",
        "      StructField('Product_Number',IntegerType(),True),\n",
        "      StructField('Order_channel_code',IntegerType(),True),\n",
        "      StructField('Date',StringType(),True),\n",
        "      StructField('Quantity',IntegerType(),True),\n",
        "      StructField('Unit_price',FloatType(),True),\n",
        "      StructField('Unit_sale_price',FloatType(),True),\n",
        "])\n",
        "\n",
        "Schema_RD = StructType([\n",
        "      StructField('Retailer_code',IntegerType(),True),\n",
        "      StructField('Retailer_name',StringType(),True),\n",
        "      StructField('Type',StringType(),True),\n",
        "      StructField('Country',StringType(),True),\n",
        "])"
      ],
      "metadata": {
        "id": "6KzT0mVsPi05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using Spark SQL read the raw data from the files systems (HDFS), as a separate data\n",
        "frames by specifying the schema for reading the files. (10 Marks)"
      ],
      "metadata": {
        "id": "fwqyq3QgQHXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OrderChannelsDF = spark.read.format('csv').option('header','true').load('/content/drive/MyDrive/Project/Dataset/Dataset/order_channels.csv')"
      ],
      "metadata": {
        "id": "8gHTIpBmDgD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OrderChannelsDF.show()"
      ],
      "metadata": {
        "id": "te3q1LAjD0pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OrderChannelsDF.printSchema()"
      ],
      "metadata": {
        "id": "ObNivzA0FtbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsOrdersDF = spark.read.format('csv').option('header','true').load('/content/drive/My Drive/Project/Dataset/Dataset/products_orders.csv')"
      ],
      "metadata": {
        "id": "CVz8qn4wD5y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsOrdersDF.show(5)"
      ],
      "metadata": {
        "id": "BR2yRoIJHRqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsOrdersDF.printSchema()"
      ],
      "metadata": {
        "id": "fYQr3ydOHSbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsDF = spark.read.format('csv').option('header','true').load('/content/drive/My Drive/Project/Dataset/Dataset/products.csv')"
      ],
      "metadata": {
        "id": "Gwo-8HxFN46Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsDF.show(5)"
      ],
      "metadata": {
        "id": "CKD_g6SNN5Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsDF.printSchema()"
      ],
      "metadata": {
        "id": "XCbCACTfOLww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RetailersDetailsDF = spark.read.format('csv').option('header','true').load('/content/drive/My Drive/Project/Dataset/Dataset/retailers_details.csv')"
      ],
      "metadata": {
        "id": "JrHC4PCYHWdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RetailersDetailsDF.show(5)"
      ],
      "metadata": {
        "id": "qaujqokbHyki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RetailersDetailsDF.printSchema()"
      ],
      "metadata": {
        "id": "LgqR_9MrHzYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF = spark.read.format('csv').option('header','true').load('/content/drive/My Drive/Project/Dataset/Dataset/sales.csv')"
      ],
      "metadata": {
        "id": "YCb0BqrQH_kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.show(5)"
      ],
      "metadata": {
        "id": "wZu-0FW2IMmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.printSchema()"
      ],
      "metadata": {
        "id": "obXrkoNBITjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Perform a basic analysis of the various data frames and store the results on to the file system\n",
        "#Basic Analysis include: Select any one DF and show the following\n",
        "# a. Display the columns names (3 Marks)"
      ],
      "metadata": {
        "id": "5_7rtAEsQXZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.columns"
      ],
      "metadata": {
        "id": "8H0wS0MUIp1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #b. Display the datatypes of the columns (3 Marks)"
      ],
      "metadata": {
        "id": "1-p_7pxNQsdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.dtypes"
      ],
      "metadata": {
        "id": "-KuEynmzI0eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#c. Find the maximum and minimum values in each column (3 Marks)"
      ],
      "metadata": {
        "id": "pGkc4vZhQ2Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.select(max('Unit_price'),max('Unit_sale_price')).show()"
      ],
      "metadata": {
        "id": "h74W6NWyJBGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.select(min('Unit_price'),min('Unit_sale_price')).show()"
      ],
      "metadata": {
        "id": "EJE1ShKcJFHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #d. Define a table/view on the spark dataframe created to run sql queries on the dataframe. (5 Marks)"
      ],
      "metadata": {
        "id": "huZVmVb7Q-wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.createOrReplaceTempView('sales_temp')\n",
        "sales_temp1=spark.sql(\"SELECT * FROM sales_temp\")\n",
        "sales_temp1.show()"
      ],
      "metadata": {
        "id": "aP1qUl-8JQx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#d. Define a table/view on the spark dataframe created to run sql queries on the dataframe. (5 Marks)"
      ],
      "metadata": {
        "id": "XY0R0YPlRJqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales=salesDF.toPandas()"
      ],
      "metadata": {
        "id": "6RVmO2kUJdfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales\n"
      ],
      "metadata": {
        "id": "Wqp_pX8BJjTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.isnull().sum()"
      ],
      "metadata": {
        "id": "3vg5Twd3JppV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the datasets by merging various data frames. Try to find the answers\n",
        "## for the questions for example:\n",
        "# f. How many orders chose a particular channel? (10 marks)"
      ],
      "metadata": {
        "id": "ckWUbU5ORXm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salesDF.groupBy('Order_channel_code').agg({'Quantity':'sum'}).show()"
      ],
      "metadata": {
        "id": "B-UXeaKEMHZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## g. Show country wise retailer names. (10 marks)"
      ],
      "metadata": {
        "id": "l0l1yiXEM5fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Retailer_sorted1=RetailersDetailsDF.select('Country','Retailer_name').show()"
      ],
      "metadata": {
        "id": "kbed13yHNAHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bQddQyQMNHQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## h. Number of products available under each project type."
      ],
      "metadata": {
        "id": "0IW5WmsaSjjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ProductsDF.groupBy('Product_type').agg({'Product':'count'}).show()"
      ],
      "metadata": {
        "id": "QAQczO1gDCA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## i. Find minimum and maximum unit_price for product_type. (5 marks)"
      ],
      "metadata": {
        "id": "dNMdur1jTXq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "ProductsDF.groupBy('Product_type').agg(F.max(ProductsDF.Unit_price),F.min(ProductsDF.Unit_price)).show()"
      ],
      "metadata": {
        "id": "Av43g8iIM32E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}